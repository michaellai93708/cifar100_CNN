# cifar100_CNN
This project aims to achieve high accuracy when recognizing images in the cifar-100 dataset. The algorithms used were SVM, KNN and CNN. The highest accuracy observed was 60% when using CNN. 
   When pre-processing the data, the team kept 80% of variance / 25 features to decrease computing complexity. The team imported the image data generator package from Keras to implement image augmentation.
   As shown in the following figure, when constructing the convolutional layers, three 2-dimensional convolutional layers were built. The first 2-dimensional convolutional layer accepted the (32, 32, 3) training data, applying 128 filters with a 3x3 convolution window. Padding was added evenly around each image. We set the activation function to Rectified Linear Unit (relu), which converts all negative input into 0. A normalization layer was added to keep the data normalized between the first 2-dimensional convolutional layer and the second 2-dimensional convolutional layer.
   The second and the third 2-dimensional convolutional layers had 64 filters and had a 3x3convolution window, the padding was set to be identical and the activation function used was relu. Normalization layers were added after each convolutional layer to normalize the data. After the normalization layer, two max-pooling layers were added with a kernel size of 3x3. Means that the 3x3 matrix would be condensed into a single value which would be the largest in the matrix. Padding for both pooling layers were set to be identical.
    After flattening the matrix, the train data were passed to three dense layers. The first layer was consisted of 512 units, using relu as an activation function. After the first dense layer, a normalization and dropout layer was added to normalize the training data. The dropout was set to be 0.5, meaning that 50% of the units would be inactivated for each training, which could theoretically improve the model's performance (Srivastava, et al., 2014). The second dense layer was consisted of 256 units, using relu as an activation function, a 50% dropout layer and a normalization layer were also added after this dense layer. For the last fully connected layer, the number of units were set to be 100, where each of the units represents one label. Softmax was used as an activation function, which could convert vectors of number into the probability of labels.
    When training the model, Adam was adopted rather than having the optimization using Stochastic gradient descent, which could theoretically accelerate computation time and lower memory usage (Kingma & Ba, 2015).
<img width="528" alt="model_summary_figure" src="https://user-images.githubusercontent.com/100908727/161408997-6506316b-a873-48f0-94ca-b6384d03d9a6.png">


For full report details, please refer to the pdf file in the same directory
